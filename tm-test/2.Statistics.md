### Аналитика

> **Дано**: Файл с временной статистикой работы асессоров над однотипным заданием.

> Задание может состоять из одного или нескольких микрозаданий. Время резервирования (assigned_ts) указывает, когда система назначила асессора исполнителем задания. Асессор мог приступить к заданию сразу же или позже (отошёл выпить чаю, а потом начал работать; выполнял предыдущее задание, в то время как за ним зарезервировали новые). Асессор за 30 секунд рабочего времени получает N рублей.

> **Необходимо описать пошаговый алгоритм для нахождения справедливой оплаты за выполнение одного микрозадания из файла. Если бы вам пришлось реализовывать данный алгоритм, какие инструменты вы бы использовали?**

Для вычисления справедливой оплаты нам нужно знать, сколько у асессоров в среднем занимает выполнение одного микротаска (предполагаем, что асессору не назначается оплата индивидуально, хотя в расчетах ниже эти цифры вычисляются). Зная эту величину (`S` секунд), мы можем высчитать и цену микротаска - `(N * S) / 30`. Забегая вперед - у меня получилось `S = 78.27`, то есть справедливая цена таска - **N * 2.61** рублей. Об этом ниже.

Просто высчитать среднее среди всех `closed_ts - assigned_ts` мы не можем: данные "зашумлены"/много выбросов, но главное, время начала задания не обязательно равно `assigned_ts`. Описанный в условии случай "асессор отошёл выпить чаю, а потом начал работать" мы отловить не можем (в данных нет времени фактического начала), но вот кейс "выполнял предыдущее задание, в то время как за ним зарезервировали новые" нам доступен. Будем считать временем начала выполнения задания максимальное `closed_ts` другого таска, пересекающегося с временным интервалом текущего.

Из инструментов будут использоваться: консоль СУБД, командная строка для заливки/выгрузки CSV-файлов, блокнот Jupyter Notebook (а в нем библиотека Pandas) для аналитики и вычислений. Для диаграмм при исследовании данных можно было бы пользоваться офисными программами (Excel/OpenOffice Calc), но мне показалось удобнее строить частотные гистограммы прямо в IPython. В реальных условиях это была бы таблица YT, срабатывающий по триггеру Хитман-процесс, и запускаемый им граф в Нирване, выполняющий YQL-запросы и обрабатывающий данные скриптами на Groovy.

Вычисления лежат а блокноте Jupyter, его можно просматривать прямо здесь, GitHub его выполнит - [ссылка](2.Statistics.ipynb)

Пошагово:

#### 1. Заливка данных в базу

Создаем таблицу в тестовой БД:

```sql
CREATE TABLE tasks (
  id serial PRIMARY KEY, "login" int, "tid" bigint,
  "microtasks" int, "assigned_ts" timestamp, "closed_ts" timestamp
);
CREATE INDEX ON tasks1 (login);
CREATE UNIQUE INDEX ON tasks1 (id, login);
```

Индексы нужны здесь из-за существенного размера данных и необходимости группировки по логину.

Как и в заданиях с SQL-запросами, скачиваем TSV-файл и заливаем данные в БД через командную сроку. Для упрощения `JOIN`-ов и экономии места подготавливаем данные: обрезаем префиксы у ID пользователей. Отбрасываем также строку заголовка:

```bash
ssed -E '1d;s/^\s*login//g' data_task4_old.tsv | psql -d test -c 'COPY tasks FROM stdin'
```

Получилось чуть больше 700 тысяч записей. Для исследования данных и тестовых запросов можно, например, скопировать несколько тысяч записей во временную таблицу.

#### 2. Очистка и исследование данных

Эксперименты с залитыми данными показывают несколько вещей:

- Всего одна "битая" временная метка:

    ```sql
    DELETE FROM tasks WHERE assigned_ts > closed_ts;
    >  DELETE 1
    ```

- В данных присутствуют таски с разными ID, но одинаковым временем выполнения. Время начала и время завершения у них либо совпадают, либо отличаются на пару секунд:

    ```sql
    SELECT * FROM tasks AS t1 INNER JOIN tasks AS t2
    ON t1.login = t2.login AND t1.tid != t2.tid AND t1.closed_ts = t2.closed_ts LIMIT 10;
    ```

    ```
    -[ RECORD 1 ]--------------------
    login       | 7
    tid         | 197183914
    microtasks  | 1
    assigned_ts | 2017-05-24 10:04:06
    closed_ts   | 2017-05-24 10:05:37
    id          | 819
    login       | 7
    tid         | 197247974
    microtasks  | 1
    assigned_ts | 2017-05-24 10:04:06
    closed_ts   | 2017-05-24 10:05:37
    id          | 820
    -[ RECORD 2 ]--------------------
    login       | 7
    tid         | 197012742
    microtasks  | 7
    assigned_ts | 2017-05-23 05:00:16
    closed_ts   | 2017-05-23 05:06:22
    id          | 39
    login       | 7
    tid         | 197042222
    microtasks  | 9
    assigned_ts | 2017-05-23 05:00:18
    closed_ts   | 2017-05-23 05:06:22
    id          | 41
    -[ RECORD 3 ]--------------------
    ...
    ```

    Таких заданий достаточно много. Не ясно, как интерпретировать такие данные. Если это не читерство, остается только признать их валидными, и учитывать в расчете времени *все* одновременно выполняемые задания. Для различения их с последовательно выполняемыми введем константу - *минимальное время выполнения* задания. Например, 30 секунд. Задания, чье время завершения различаются хотя бы на 30 секунд, участвуют в расчете перекрывающихся интервалов. Иначе, их время учитывается целиком, без перекрытия. Строим запрос.

#### 3. Сбор данных

Здесь `extract(epoch FROM ...)` - лишь способ, каким PostgreSQL переводит даты в секунды:

```sql
SELECT
  t1.login,
  extract(epoch FROM (t1.closed_ts - coalesce(max(t2.closed_ts), t1.assigned_ts))/t1.microtasks)
FROM
  tasks AS t1
LEFT JOIN
  tasks AS t2
ON
  t1.tid != t2.tid
  AND t1.login = t2.login
  AND t2.closed_ts BETWEEN t1.assigned_ts AND t1.closed_ts
  AND extract(epoch FROM t1.closed_ts - t2.closed_ts) > 30
GROUP BY
  t1.id, t1.login;
```

Учитывая размер таблицы, `JOIN` (тем более `LEFT`) и агрегация выполняются довольно долго, но на слабом компьютере укладываются в примерно 3 минуты.

Самые важные моменты:
- мы делаем `LEFT JOIN` на себя, чтобы отловить другие задания, выполняемые пользователем после того, как на него было назначено текущее
- если таких заданий нет, считаем что идет выполнение без перекрытий, и за время начала берем `t1.assigned_ts`
- если же условие `t2.closed_ts BETWEEN t1.assigned_ts AND t1.closed_ts` выполнено, и такие (перекрывающие) задания есть, берем то, чье время завершения максимально - `max(t2.closed_ts)`
- получаем *фактическое* время, которое было у асессора после завершения одного и до завершения другого (текущего) задания - `t1.closed_ts - coalesce(max(t2.closed_ts), t1.assigned_ts)`
- переводим его в секунды и делим на количество микротасков в этом задании

Построенный запрос оборачиваем в команду импорта

```sql
\copy (SELECT ...) to './output.csv' with CSV;
```

и на выходе получаем [CSV-файл](output.csv) из двух столбцов (<логин>-<среднее-время-на микротаск-в-секундах>) и 700 тысяч строк, по одной на каждое задание. Размером ~5.8 мегабайт.

#### 4. Удаление выбросов и анализ

В принципе, все наглядно видно (и есть пояснения) в лежащем в этой же папке блокноте Jupyter - [ссылка](2.Statistics.ipynb). Кратко:
- подключаем `pandas`, импортируем данные из CSV-файла во фрейм
- отсекаем "хвосты" по пятому и девяносто пятому процентилям
- в процессе строим диаграммы для наглядности
- группируем данные по логину, и в каждой группе фильтруем значения по правилу трех сигм
- после этого агрегируем очищенные значения, вычисляя их мат. ожидание
- окончательно очищаем ставший уже одномерным массив средних, и вычисляем на нем "среднее средних" - наше искомое время - **78.27** секунд

#### 5. Итог

Разумеется, оценка сильно приблизительная. Использование других квантилей, других значений для среднего, других методов очистки и удаления выбросов, либо даже не-использование их так часто, привели бы к другим значениям среднего времени.

Что более важно, методика расчета "справедливого" вознаграждения может быть и вовсе не такой. Я делал вычисления в предположении, что это будет некое среднее - время, усредненное сначала по всем таскам асессора, а потом усредненное и по всем асессорам. Алгоритм расчета, исходящий из других предпосылок, может получать сильно отличающиеся результаты.

Стоит сказать, что если не засчитать все одновременно выполняемые таски (см. проблему, описанную в пункте 2), просто отбросив их, то финальный результат - среднее время - уменьшится на 21.7%, до 61.28 секунд 
