#### Введение
- большие данные - такие, которые нельзя обработать традиционными средствами (напр., РСУБД). Характеризуются тремя (или четырьмя) V:
  * volume - объем, сколько данных содержит набор
  * variety - насколько отличаются друг от друга разные типы данных
  * velocity - с какой скоростью генерируются новые данные
  * (иногда) veracity - насколько данные точны
- data science добавляет к статистике методы computer science, и этим от нее отличается. Статистика + большие данные + подготовка в области CS / алгоритмы = Data Science
- Python в DS используется потому что прост, и потому что под него много библиотек (напр., есть для всех NoSQL-баз)
- описывает области применения DS, [p.20]
- типы данных:
  * структурированные - в основном таблицы, базы данных и Excel. SQL - основное средство управления такими данными
  * неструктурированные - их нельзя подогнать под определенную модель. Пример - email-ы
  * на естественном языке - частный случай неструктурированных данных, email-ы также являются его хорошим примером
  * машинные - автоматически генерируемые (компьютерами) данные. Пример - логи серверов, журналы телеметрии и прочее
  * графовые (сетевые) - в них главную роль играют отношения между сущностями. Пример - участники и их отношения (друзья, группы) в соцсетях. Существуют спец. графовые базы данных и специализированные языки запросов для работы с ними, типа SPARQL
  * аудио, видео и графика (медиа?) - задачи распознавания образов
  * потоковые - не загружаются в систему одним большим объемом, а приходят постепенно. Примеры - прямые трансляции в Твиттере, биржевые котировки
- процесс DS часто бывает итеративным, с возвратом к предыдущим шагам:
  1. назначение цели исследования - т.к. процесс DS применяется в основном в контексте организаций, здесь ставится проектное задание, указывается какую выгоду это принесет, каких ресурсов потребует и в какие сроки будет выполнено. Все участники должны понимать "что", "как" и "почему"
  2. сбор данных - доступ к ресурсам из пред. пункта. На выходе получаются "сырые" данные
  3. подготовка данных - три фазы: очистка (устранение ошибок и расхождений между источниками), интеграция (сведение источников), преобразование (приведение к формату, подходящему для использования в моделях)
  4. исследование данных - понимание сути данных и того, как переменные взаимодействуют друг с другом. Описательные статистики, визуальные методы и простое моделирование
  5. моделирование данных - построение модели предметной области для получения ответа на вопрос исследования. Основной этап. Здесь часто несколько простых моделей работают лучше одной сложной
  6. отображение и автоматизация - презентации и отчеты для предоставления бизнес-стороне. Для возможности повторения исследования его можно автоматизировать
- экосистема больших данных (большая схема):
  * распределенные файловые системы - работают на нескольких серверах одновременно, скрывают детали реализациии от пользователя, способны хранить файл больше размера диска одного компьютера, хорошо масштабируются. Самая популярная - HDFS (Hadoop FS)
  * распределенное программирование - "перемещение программы к данным". Перезапуск сбойных заданий, отслеживание результатов из других субпроцессов и т.д.
  * интеграция данных - загрузка/выгрузка данных в распределенные ФС. Такие технологии как Apache Sqoop / Apache Flume
  * машинное обучение - извлечение скрытой информации. Библиотеки Python: Scikit-learn, PyBrain (нейронные сети), NLTK (Natural Language ToolKit), Pylearn2 (похожа, но уступает Scikit-learn), TensorFlow (глубокое обучение от гугл). Есть и не только Python-библиотеки, напр., Spark - ядро машинного обучения от Apache
  * NoSQL БД - "No" означает "не только". Хорошо масштабируются для распределенной работы, и могут обрабатывать разные типы данных (см. выше). Бывают: столбцовые (?), хранилища документов (не таблицы), потоковые (напр., Apache Storm), хранилища ключ-значение, SQL в Hadoop (map-reduce на языке запросов), обновленный SQL (?), графовые
  * инструменты планирования - аналоги CRON для распределенных систем
  * инструменты сравнительного анализа - профилирование распределенных систем, задача сисадминов
  * развертывание систем - то же, не наша задача
  * программирование служб - создание API и REST-сервисов для предоставления доступа к результатам исследования
  * безопасность доступа к данным - опять же, не совсем наша задача, в книге не рассматривается
- пример использования Hadoop через образ виртуальной машины, [p.34]. HiveQL - диалект SQL, используется для выборки данных в Hadoop, преобразуется в задание MapReduce

#### Процесс data science
- в главе идет разбор процесса по шагам. Эти 6 шагов кратко указаны выше (см. нумерованный список в первой главе)
- известная поговорка в DS: "мусор на входа - мусор на выходе", так что лучше соблюдать эту структуру действий из шести этапов
- противопоставляет agile как гибкий подход этой структуре с фиксированными этапами, и пишет что agile не всегда возможен

&nbsp;

1. Определение целей исследования и создание проектного задания
   * что хочет компания, почему исследование важно
   * задавать как модно больше вопросов и изобретать примеры, иначе есть риск после месяцев исследований узнать, что поставленная цель была понята неправильно
   * в общем, уделять больше внимания требованиям и желаниям бизнеса
   * проектное задание должно включать цель, предварительную методику исследования, требуемые ресурсы, какие результаты будут предъявлены и календарный план

2. Сбор данных
   * данные компаний бывают внутренние и внешние
   * начните с внутренних, они могут быть как обработанные, так и сырые, могут хранится как в базах данных, так и на компьютере эксперта в виде Excel-файлов
   * получение прав доступа к данным - другая сложная задача
   * не бойтесь получать данные из внешних источников, как бесплатно так и платно. Список некоторых сайтов, предлагающих открытые данные, [p.49]
   * уделить больше внимания проверке качества данных: очистка занимает до 80% процентов времени проекта, нужно проверять всё как можно раньше. На этом этапе проверки механические - опечатки, форматы, написание и т.д.

3. Очистка, интеграция и преобразование данных
   * очистка (устранение *ошибок интерпретации* и *расхожений*)
     - ошибки ввода данных. Составление частотных таблиц и замена значений по `if` поможет устранить их
     - избыточные пробелы. `strip()`
     - расхождения в регистре символов. `lower()`
     - невозможные значения и проверка разумности (напр., возраст). Через `if`
     - выбросы (заметно отклоняющийся результат). Проверка по диаграммам или таблицам с min/max
     - отсутствующие значения. Исключение их, присваивание им `null`/`0`/среднего арифм., моделирование значения по общему распределению и пр.
     - разные единицы измерения. Простые преобразования
     - разные уровни агрегирования. Сведение/расширение (?) наборов данных
   * исправляйте ошибки в данных как можно раньше. Ведь на ошибочных данных могут приниматься ошибочные стратегии, эти данные могут попадать в несколько проектов, приводить к критичным для компании ошибкам
   * лучше исправлять данные сразу после их получения. Если это невозможно, то исправления нужно делать в своем коде. Всегда храните копию исходных данных
   * комбинирование данных из разных источников (зд. только про табличные данные)
     - соединение таблиц (join), по некоторому ключу
     - дополнение таблиц (union), записи из таблиц с одинаковой структурой сливаются в одну
     - для экономии дискового пространства, в обоих случаях можно строить view - виртуальную имитацию физической таблицы. Но view строится при каждом запросе, так что это увеличивает расход вычислительных ресурсов
     - создание новых таблиц со сводными метриками. Строим запросы для вычисления, напр., средних значений, и сохраняем их как (виртуальную) таблицу
   * преобразование данных
     - пример с переходом от экспоненциальной к логарифмической зависимости между x и y делает эту зависимость линейной (??)
     - выделение главных компонент (переменных), рассматривается дальше
     - зачем-то вводит формулу евклидова расстояния
     - переход к вспомогательным (dummy) переменным, которые принимают значения 0/1. Напр., преобразование столбца "Gender" со значениями M/F в два столбца Male/Female со значениями 1/0

4. Исследовательский анализ данных
   * углубленное изучение данных, применяются в основном графические методы
   * простые графики, столбцовые диаграммы, кривые распределения, диаграммы Сэнки, сетевые графы, интерактивные и анимированные диаграммы, наложения диаграмм, диаграммы Парето, гистограммы, коробчатые диаграммы
   * метод связывания и пометки данных - выбор данных на одной диаграмме выделяет их сразу и на других (???)
   * но могут быть и не только визуальные методы, а, напр., сведение в таблицы или кластеризация

5. Построение моделей
   * выбор модели и переменных
   * выполнение модели
     - ссылка на приложение Г - настройка conda
     - пример линейной регрессии на Python с использованием numpy (генерация случайных точек в двумерном пространстве) и statmodels (подбор линейной регрессии и вывод таблицы результата)
     - *(adjusted) R-squared* - степень соответствия модели, степень разброса данных. Хорошие модели имеют ее > 0.85-0.9
     - *coef* - коэфф. свободных переменных, насколько изменение x на 1 изменяет y. Влияение сводобной переменной на целевую, коэфф. линейного уравнения. Показывает, что (и насколько) x является причиной влияния на y
     - *P>|t|* - значимость влияния пред. коэффициента. Что-то про ошибки 1го и 2го типов. Должна быть < 0.05-0.1
     - пример классификации, метод k ближайших соседей, на Python с numpy (то же) и Scikit (а в примере sklearn???) (классификация)
     - *confusion matrix* - матрица несоответствия, по диагонали кол-во правильно спрогнозированных случаев
     - в обоих случаях модель не проверялась на свежих данных - *holdout sample* (контрольной выборке), о которой рассказывается позже
     - в Python есть лишь немного готовых моделей, но всегда можно воспользоваться библиотекой RPy, дающей интерфейс в язык R, где моделей больше
   * диагностика и сравнение моделей
     - в ходе анализа может быть построено несколько моделей, все они проверяются контрольной выборкой
     - напр., 80% данных используются для тренироки модели, и 20% - для проверки
     - для каждой модели вычисляется *среднеквадратичная погрешность* (сумма квадратов отклонений прогноза от истинного значения), по ней выбирается лучшая модель

6. Представление результатов и построение приложений на их основе
   * оно очень важно, да
   * автоматизация может автоматически обновлять таблицы Excel с результатами или презентации PowerPoint

#### Машинное обучение
- машоб - область исследований, наделяющая компьютеры возможностями, не заложенными в них изначально
- основные задачи машоба в DS - регрессии и классификации, есть еще кластеризация
- целью построения модели может быть как прогнозирование, так и интерпретация (анализ первопричин)
- инструменты Python, используемые в DS
  * для работы с данными в памяти:
    - SciPy просто сборник популярных пакетов, таких как NumPy, matplotlib, Pandas и SymPy
    - NumPy - массивы и функции линейной алгебры
    - Matplotlib - научная 2D-графика и немного 3D
    - Pandas - вводит концепцию *фреймов данных* (что-то вроде таблиц в памяти)
    - SymPy - символьная математика и компьютерная алгебра
    - StatModels - модели и алгоритмы статистики
    - Scikit-learn - алгоритмы машинного обучения
    - RPy2 - вызов функций R из Python-кода
    - NLTK - анализ текста
  * оптимизация операций (ускорение приложений в проде)
    - Numba[Pro] - JIT компиляция для аннотированного Python-кода + GPU
    - PyCUDA - GPU
    - Cython (C для Python) - статическая типизация и привнесение языка C (?)
    - Blaze - работа с большими данными
    - Dispy и IPCluster - написание кода для работы в кластерах
    - PP - параллельные вычисления на одном компьютере или в кластерах
  * большие данные
    - Pydoop и Hadoopy - интеграция с Hadoop
    - PySpark - интеграция со Spark (работа с большими данными в памяти)
- возможно комбинирование моделей - они обучаются независимо, и потом выход одной подается на вход другой. Это называется *ensemble learning* (композиционным обучением)
- модель состоит из *показателей* или *свободных переменных* и *целевой переменной*. Прогнозирование последней является основной целью модели
- процесс моделирования
  * планирование показателей и выбор модели. В учебным примерах свободные переменные явно берутся из датасета, в реальных их часто приходится искать самому, иногда даже создавая отдельные модели. *Субъективное смещение* - ошибка, когда аналитик выбирает те переменные, которые легче доступны (пример с самолетами и пробитыми крыльями)
  * тренировка, как правило всё есть в библиотеках
  * проверка адекватности модели. Две популярные метрики: *частота ошибок классификации* (% неправильных классификаций в тестовом датасете) и *среднеквадратичная погрешность* для регрессий. Последняя использует возведение в квадрат чтобы разнонаправленные ошибки не компенсировали друг друга, и чтобы большие ошибки имелли больший вес
  * применение к незнакомым данным
- способы проверки адекватности модели
  * разбиение датасета на тренировочный и контрольный наборы (используется чаще всего)
  * K-кратная перекрестная проверка разбивает датасет на K частей, потом одна используется как тренировочная, остальные как проверочные (?)
  * исключение единицы, предыдущий метод с K=1. Только с малыми наборами данных, применим мало
  * как-то относится к этому делу и *регуляризация* - внесение штрафа за каждую свободную переменную. L1-регуляризация подразумевает модель с минимумом свободных переменных, L2 направлена на минимизацию расхождений между коэффициентами сободных переменных (?)
- типы машинного обучения
  * контролируемое - на размеченном датасете
    - пример - распознавание цифр (капча)
    - наивный байесовский классификатор, на примере сначала почтового спама, а потом распознавания цифр MNIST с использованием библиотеки sklearn
    - sklearn дает и датасет, и функцию `reshape()` (приведение к одномерному массиву), и байесовский классификатор, и результат (см. след. пункт)
    - *confusion matrix* (матрица несоответствий) по главной диагонали имеет число совпадений (`i` с `j`) - правильный детект цифры - истинное срабатывание, а остальные элементы в основном ноль (в общем случае - сколько раз детектилось `j`, хотя на самом деле было `i`) - срабатывания ложные
    - эта матрица квадратная, кол-во ее строк/столбцов равно числу категорий классификатора. В простейшем случае ("да"/"нет") их две, в случае цифр - 10
    - сумма элементов главной диагонали (правильные срабатывания) называется *следом матрицы*. Желательно, чтобы она была высока по сравнению с суммой остальных элементов
    - можно выводить некие "отладочные" изображения цифр (то, как их видит программа), распознавать и использовать снова в обучении модели, но не совсем понятно как (?)
  * неконтролируемое - на неразмеченном
    - большие датасеты часто неразмечены, поэтому исследуют или распределение данных, или их структуру и значения
    - переменные в датасете делятся на *observable* (наблюдаемые) и *latent* (скрытые). Сведение большого числа наблюдаемых переменных к меньшему числу скрытых - очень полезный навык: снижается размерность, упрощается работа
    - PCA, Principal Components Analysis (метод главных компонент) предназначен как раз для поиска скрытых переменных. Дается пример на Python, исследование качества вина. Библиотека Scikit-learn, датасет от Калифорнийского университета в Ирвайне (UCI)
    - *scree plot* ("график каменистой осыпи") демонстрирует вклад каждой (скрытой) переменной, и является неотъемлемой частью PCA
    - выделенным из этого графика компонентам нужно дать имена, и вероятно для этого требуются экспертные знания в предметной области
    - строим фрейм данных Pandas с новыми (скрытыми) переменными, и проверяем их в прогнозировании, через байесовский классификатор из того же sklearn, как в пред. примере
    - другим методом является *кластеризация*. В модуле sklearn.cluster есть несколько алгоритмов, включая метод k-средних, алгоритм affinity popogation (распространения близости) и спектральныю кластеризацию. Есть (тоже там?) и методы иерархической кластеризации. Недостатком является то, что во всех методов нужно задавать число кластеров заранее, и потом идти методом проб и ошибок
    - пример кластеризации метод k-средних датасета с ирисами, смешивающий неконтролируемое и контролируемое обучение (?)
  * смешанный вариант, частично контролируемое
    - когда меток в датасете очень мало
    - в этом случае можно применить метод *распространения меток* - кластеризовать переменные и назначить всем в кластере то же значение, что и у размеченной
    - еще какой-то метод *активного обучения*, когда алгоритм итеративно указывает, какие наблюдения должны быть помечены на следующем шаге. Или что-то в таком роде

#### Работа с данными на одном компьютере
- основными проблемами при работе с big data являются нехватка памяти, работающие вечно процессы, "узкие места" (одни компоненты перегружены, другие простаивают) и низкая скорость
- решения этих проблем делятся на три группы, три правильных выбора
  * алгоритма. Такой алгоритм не обязан загружать весь датасет в память
    - онлайновые алгоритмы, подход "использовал и забыл". Пример перцептрона на одной NumPy, обучение по эпохам. Бывают и полно- и мини-пакетные, когда данные поступают частями. Отличаются от потоковых алгоритмов тем, что могут несколько раз получать доступ к одним и тем же данным
    - блочные алгоритмы базируются на том, что действия с матрицами (сложение, умножение, инверсия и сингулярное разложение) можно проводить по частям: разбить матрицы на блоки и оперировать уже ими. Такие алгоритмы как PCA и линейная регрессия могут быть представлены в матричной форме. Библиотеки Python: bcolz оптимизирует хранение массивов данных, Dask - оптимизирует последовательность вычислений и распараллеливает их. Пример с этими библиотеками
    - MapReduce, отображение-свертка. Сначала производит отображение набора значений на ключ (map), а потом объединение знаяений по этому ключу (reduce). Б*о*льшую часть работы на себя берут библиотеки Python, такие как Hadoopy, Octopy, Disco, Dumbo
  * структуры данных
    - разреженные данные, напр., матрицы, где большинство элементов равно 0. Часто получаются в реальных задачах (напр., в сообщениях твиттер, в формате "содержит"/"не содержит" слово, почти везде будет "не содержит", 0), и занимают много памяти. Их можно сжимать, напр., до списка кортежей `[(<row>, <col>, <val>), ...]`. Поддержка разреженных матриц в Python растет
    - деревья существенно ускоряют поиск/выборку данных. Используются, например, для построения индексов баз данных
    - хеш-таблицы также ускоряют выборку и также используются для индексов БД. Близкие родственники хранилищ "ключ-ззначение". Пишет, что ключи объединяются в какие-то *buckets* ("гнезда")
  * инструментов, зд. для Python
    - Cuthon, статическая типизация
    - Numexpr, аналог NumPy (там данные в памяти) для больших данных. Использует JIT-компиляцию
    - Numba - просто JIT-компиляция Python-кода
    - Bcolz решает проблемы NumPy при использовании больших данных. Сжимает массивы, неявно используя Numexpr
    - Blaze неявно преобразует данные в стиле Python в SQL, в формат реляционных баз данных или других, типа CSV или Spark
    - Theano, символические вычисления и тензоры. Поддержка GPU и JIT-компиляции
    - Dask - оптимизация последовательности вычислений и их распараллеливание
    - кроме того, Python имеет интерфейсы ко многим популярным программным продуктам
- общие рекомендации при работе с big data в основном те же, что и для общего программирования
  * не повторяйте уже сделанную работу. Используйте базы данных и готовые (высокооптимизированные) библиотеки
  * используйте все возможности оборудования. Передавайте процессору сжатые данные, используйте видеокарту, используйте многопоточные параллельные вычисления
  * экономьте вчислительные ресурсы. Профилируйте и оптимизируйте узкие места, используйте откомпилированный код (из пакетов или свой, через JIT/другие языки), избегайте загрузки всех данных в память (читайте их блоками/последовательно), используйте генераторы для отказа от промежуточного хранения данных (а это не то же, что пред.?), используйте как можно меньший объем данных (?), преобразовывайте математические выражения (пример с квадратом сумммы)
- пример 1, прогнозирование вредоносных урлов
  * SciKit-learn и большой внешний датасет уже на стадии загрузки вызывают нехватку памяти
  * простой строчкой кода на Python проверяем, что данные содержат всего десятитысячные доли процента ненулевых значений
  * т.о. для экономии памяти работаем сразу с разреженным данными - что-то там про формат SVMLite, который их по дефоллту поддерживает. Кроме того, через библиотеку `tarfile` перебираем файлы в датасете, не распаковывая его
  * используем (опять же, для экономии памяти) онлайновый алгоритм стохастического градиентного спуска, вычитывая данные из файлов по частям, через `partial_fit()`
  * функция `classification_report()` выводит таблицу с точностью, полнотой и прочим
- пример 2, рекомендательная система внутри базы данных
  * MySQL (установка описана в прил. B) и библиотека Python для интеграции с ней (используется MySQLdb, но можно и SQLAlchemy)
  * использует метод k ближайших соседей, локально-чувствительное хеширование (LSH), и расстояние Хемминга для строк
  * в матрице (в таблице БД) "клиент"/"смотрел ли фильм" столбцы фильмов (0/1) можно объединить в один, в битовое поле (`1100101100...`). Это преобразование он и называет хеш-функцией (?)
  * в процессе работы с данными зачем-то используются фреймы Pandas
  * для ускорения работы прямо через Python строит индекс на таблице
  * вычисление расстояния Хемминга реализует в виде хранимой процедуры

#### Введение в big data
- 

---

- Hadoop, HiveQL
- настройка conda
- примеры на Python [pp.73,75-76,94-97,102-107,110,118-122,124-125,135-138,143-149]
- байесовская вероятность и правило
- библиотеки Python, примеры на которые есть в книге
