#### Введение
- большие данные - такие, которые нельзя обработать традиционными средствами (напр., РСУБД). Характеризуются тремя (или четырьмя) V:
  * volume - объем, сколько данных содержит набор
  * variety - насколько отличаются друг от друга разные типы данных
  * velocity - с какой скоростью генерируются новые данные
  * (иногда) veracity - насколько данные точны
- data science добавляет к статистике методы computer science, и этим от нее отличается. Статистика + большие данные + подготовка в области CS / алгоритмы = Data Science
- Python в DS используется потому что прост, и потому что под него много библиотек (напр., есть для всех NoSQL-баз)
- описывает области применения DS, [p.20]
- типы данных:
  * структурированные - в основном таблицы, базы данных и Excel. SQL - основное средство управления такими данными
  * неструктурированные - их нельзя подогнать под определенную модель. Пример - email-ы
  * на естественном языке - частный случай неструктурированных данных, email-ы также являются его хорошим примером
  * машинные - автоматически генерируемые (компьютерами) данные. Пример - логи серверов, журналы телеметрии и прочее
  * графовые (сетевые) - в них главную роль играют отношения между сущностями. Пример - участники и их отношения (друзья, группы) в соцсетях. Существуют спец. графовые базы данных и специализированные языки запросов для работы с ними, типа SPARQL
  * аудио, видео и графика (медиа?) - задачи распознавания образов
  * потоковые - не загружаются в систему одним большим объемом, а приходят постепенно. Примеры - прямые трансляции в Твиттере, биржевые котировки
- процесс DS часто бывает итеративным, с возвратом к предыдущим шагам:
  1. назначение цели исследования - т.к. процесс DS применяется в основном в контексте организаций, здесь ставится проектное задание, указывается какую выгоду это принесет, каких ресурсов потребует и в какие сроки будет выполнено. Все участники должны понимать "что", "как" и "почему"
  2. сбор данных - доступ к ресурсам из пред. пункта. На выходе получаются "сырые" данные
  3. подготовка данных - три фазы: очистка (устранение ошибок и расхождений между источниками), интеграция (сведение источников), преобразование (приведение к формату, подходящему для использования в моделях)
  4. исследование данных - понимание сути данных и того, как переменные взаимодействуют друг с другом. Описательные статистики, визуальные методы и простое моделирование
  5. моделирование данных - построение модели предметной области для получения ответа на вопрос исследования. Основной этап. Здесь часто несколько простых моделей работают лучше одной сложной
  6. отображение и автоматизация - презентации и отчеты для предоставления бизнес-стороне. Для возможности повторения исследования его можно автоматизировать
- экосистема больших данных (большая схема):
  * распределенные файловые системы - работают на нескольких серверах одновременно, скрывают детали реализациии от пользователя, способны хранить файл больше размера диска одного компьютера, хорошо масштабируются. Самая популярная - HDFS (Hadoop FS)
  * распределенное программирование - "перемещение программы к данным". Перезапуск сбойных заданий, отслеживание результатов из других субпроцессов и т.д.
  * интеграция данных - загрузка/выгрузка данных в распределенные ФС. Такие технологии как Apache Sqoop / Apache Flume
  * машинное обучение - извлечение скрытой информации. Библиотеки Python: Scikit-learn, PyBrain (нейронные сети), NLTK (Natural Language ToolKit), Pylearn2 (похожа, но уступает Scikit-learn), TensorFlow (глубокое обучение от гугл). Есть и не только Python-библиотеки, напр., Spark - ядро машинного обучения от Apache
  * NoSQL БД - "No" означает "не только". Хорошо масштабируются для распределенной работы, и могут обрабатывать разные типы данных (см. выше). Бывают: столбцовые (?), хранилища документов (не таблицы), потоковые (напр., Apache Storm), хранилища ключ-значение, SQL в Hadoop (map-reduce на языке запросов), обновленный SQL (?), графовые
  * инструменты планирования - аналоги CRON для распределенных систем
  * инструменты сравнительного анализа - профилирование распределенных систем, задача сисадминов
  * развертывание систем - то же, не наша задача
  * программирование служб - создание API и REST-сервисов для предоставления доступа к результатам исследования
  * безопасность доступа к данным - опять же, не совсем наша задача, в книге не рассматривается
- пример использования Hadoop через образ виртуальной машины, [p.34]. HiveQL - диалект SQL, используется для выборки данных в Hadoop, преобразуется в задание MapReduce

#### Процесс data science
- в главе идет разбор процесса по шагам. Эти 6 шагов кратко указаны выше (см. нумерованный список в первой главе)
- известная поговорка в DS: "мусор на входа - мусор на выходе", так что лучше соблюдать эту структуру действий из шести этапов
- противопоставляет agile как гибкий подход этой структуре с фиксированными этапами, и пишет что agile не всегда возможен

&nbsp;

1. Определение целей исследования и создание проектного задания
   * что хочет компания, почему исследование важно
   * задавать как модно больше вопросов и изобретать примеры, иначе есть риск после месяцев исследований узнать, что поставленная цель была понята неправильно
   * в общем, уделять больше внимания требованиям и желаниям бизнеса
   * проектное задание должно включать цель, предварительную методику исследования, требуемые ресурсы, какие результаты будут предъявлены и календарный план

2. Сбор данных
   * данные компаний бывают внутренние и внешние
   * начните с внутренних, они могут быть как обработанные, так и сырые, могут хранится как в базах данных, так и на компьютере эксперта в виде Excel-файлов
   * получение прав доступа к данным - другая сложная задача
   * не бойтесь получать данные из внешних источников, как бесплатно так и платно. Список некоторых сайтов, предлагающих открытые данные, [p.49]
   * уделить больше внимания проверке качества данных: очистка занимает до 80% процентов времени проекта, нужно проверять всё как можно раньше. На этом этапе проверки механические - опечатки, форматы, написание и т.д.

3. Очистка, интеграция и преобразование данных
   * очистка (устранение *ошибок интерпретации* и *расхожений*)
     - ошибки ввода данных. Составление частотных таблиц и замена значений по `if` поможет устранить их
     - избыточные пробелы. `strip()`
     - расхождения в регистре символов. `lower()`
     - невозможные значения и проверка разумности (напр., возраст). Через `if`
     - выбросы (заметно отклоняющийся результат). Проверка по диаграммам или таблицам с min/max
     - отсутствующие значения. Исключение их, присваивание им `null`/`0`/среднего арифм., моделирование значения по общему распределению и пр.
     - разные единицы измерения. Простые преобразования
     - разные уровни агрегирования. Сведение/расширение (?) наборов данных
   * исправляйте ошибки в данных как можно раньше. Ведь на ошибочных данных могут приниматься ошибочные стратегии, эти данные могут попадать в несколько проектов, приводить к критичным для компании ошибкам
   * лучше исправлять данные сразу после их получения. Если это невозможно, то исправления нужно делать в своем коде. Всегда храните копию исходных данных
   * комбинирование данных из разных источников (зд. только про табличные данные)
     - соединение таблиц (join), по некоторому ключу
     - дополнение таблиц (union), записи из таблиц с одинаковой структурой сливаются в одну
     - для экономии дискового пространства, в обоих случаях можно строить view - виртуальную имитацию физической таблицы. Но view строится при каждом запросе, так что это увеличивает расход вычислительных ресурсов
     - создание новых таблиц со сводными метриками. Строим запросы для вычисления, напр., средних значений, и сохраняем их как (виртуальную) таблицу
   * преобразование данных
     - пример с переходом от экспоненциальной к логарифмической зависимости между x и y делает эту зависимость линейной (??)
     - выделение главных компонент (переменных), рассматривается дальше
     - зачем-то вводит формулу евклидова расстояния
     - переход к вспомогательным (dummy) переменным, которые принимают значения 0/1. Напр., преобразование столбца "Gender" со значениями M/F в два столбца Male/Female со значениями 1/0

4. Исследовательский анализ данных
   * углубленное изучение данных, применяются в основном графические методы
   * простые графики, столбцовые диаграммы, кривые распределения, диаграммы Сэнки, сетевые графы, интерактивные и анимированные диаграммы, наложения диаграмм, диаграммы Парето, гистограммы, коробчатые диаграммы
   * метод связывания и пометки данных - выбор данных на одной диаграмме выделяет их сразу и на других (???)
   * но могут быть и не только визуальные методы, а, напр., сведение в таблицы или кластеризация

5. Построение моделей
   * выбор модели и переменных
   * выполнение модели
     - ссылка на приложение Г - настройка conda
     - пример линейной регрессии на Python с использованием numpy (генерация случайных точек в двумерном пространстве) и statmodels (подбор линейной регрессии и вывод таблицы результата)
     - *(adjusted) R-squared* - степень соответствия модели, степень разброса данных. Хорошие модели имеют ее > 0.85-0.9
     - *coef* - коэфф. свободных переменных, насколько изменение x на 1 изменяет y. Влияение сводобной переменной на целевую, коэфф. линейного уравнения. Показывает, что (и насколько) x является причиной влияния на y
     - *P>|t|* - значимость влияния пред. коэффициента. Что-то про ошибки 1го и 2го типов. Должна быть < 0.05-0.1
     - пример классификации, метод k ближайших соседей, на Python с numpy (то же) и Scikit (а в примере sklearn???) (классификация)
     - *confusion matrix* - матрица несоответствия, по диагонали кол-во правильно спрогнозированных случаев
     - в обоих случаях модель не проверялась на свежих данных - *holdout sample* (контрольной выборке), о которой рассказывается позже
     - в Python есть лишь немного готовых моделей, но всегда можно воспользоваться библиотекой RPy, дающей интерфейс в язык R, где моделей больше
   * диагностика и сравнение моделей
     - в ходе анализа может быть построено несколько моделей, все они проверяются контрольной выборкой
     - напр., 80% данных используются для тренироки модели, и 20% - для проверки
     - для каждой модели вычисляется *среднеквадратичная погрешность* (сумма квадратов отклонений прогноза от истинного значения), по ней выбирается лучшая модель

6. Представление результатов и построение приложений на их основе
   * оно очень важно, да
   * автоматизация может автоматически обновлять таблицы Excel с результатами или презентации PowerPoint

#### Машинное обучение
- машоб - область исследований, наделяющая компьютеры возможностями, не заложенными в них изначально
- основные задачи машоба в DS - регрессии и классификации, есть еще кластеризация
- целью построения модели может быть как прогнозирование, так и интерпретация (анализ первопричин)
- инструменты Python, используемые в DS
  * для работы с данными в памяти:
    - SciPy просто сборник популярных пакетов, таких как NumPy, matplotlib, Pandas и SymPy
    - NumPy - массивы и функции линейной алгебры
    - Matplotlib - научная 2D-графика и немного 3D
    - Pandas - вводит концепцию *фреймов данных* (что-то вроде таблиц в памяти)
    - SymPy - символьная математика и компьютерная алгебра
    - StatModels - модели и алгоритмы статистики
    - Scikit-learn - алгоритмы машинного обучения
    - RPy2 - вызов функций R из Python-кода
    - NLTK - анализ текста
  * оптимизация операций (ускорение приложений в проде)
    - Numba[Pro] - JIT компиляция для аннотированного Python-кода + GPU
    - PyCUDA - GPU
    - Cython (C для Python) - статическая типизация и привнесение языка C (?)
    - Blaze - работа с большими данными
    - Dispy и IPCluster - написание кода для работы в кластерах
    - PP - параллельные вычисления на одном компьютере или в кластерах
  * большие данные
    - Pydoop и Hadoopy - интеграция с Hadoop
    - PySpark - интеграция со Spark (работа с большими данными в памяти)
- возможно комбинирование моделей - они обучаются независимо, и потом выход одной подается на вход другой. Это называется *ensemble learning* (композиционным обучением)
- модель состоит из *показателей* или *свободных переменных* и *целевой переменной*. Прогнозирование последней является основной целью модели
- процесс моделирования
  * планирование показателей и выбор модели. В учебным примерах свободные переменные явно берутся из датасета, в реальных их часто приходится искать самому, иногда даже создавая отдельные модели. *Субъективное смещение* - ошибка, когда аналитик выбирает те переменные, которые легче доступны (пример с самолетами и пробитыми крыльями)
  * тренировка, как правило всё есть в библиотеках
  * проверка адекватности модели. Две популярные метрики: *частота ошибок классификации* (% неправильных классификаций в тестовом датасете) и *среднеквадратичная погрешность* для регрессий. Последняя использует возведение в квадрат чтобы разнонаправленные ошибки не компенсировали друг друга, и чтобы большие ошибки имелли больший вес
  * применение к незнакомым данным
- способы проверки адекватности модели
  * разбиение датасета на тренировочный и контрольный наборы (используется чаще всего)
  * K-кратная перекрестная проверка разбивает датасет на K частей, потом одна используется как тренировочная, остальные как проверочные (?)
  * исключение единицы, предыдущий метод с K=1. Только с малыми наборами данных, применим мало
  * как-то относится к этому делу и *регуляризация* - внесение штрафа за каждую свободную переменную. L1-регуляризация подразумевает модель с минимумом свободных переменных, L2 направлена на минимизацию расхождений между коэффициентами сободных переменных (?)
- типы машинного обучения
  * контролируемое - на размеченном датасете
    - пример - распознавание цифр (капча)
    - наивный байесовский классификатор, на примере сначала почтового спама, а потом распознавания цифр MNIST с использованием библиотеки sklearn
    - sklearn дает и датасет, и функцию `reshape()` (приведение к одномерному массиву), и байесовский классификатор, и результат (см. след. пункт)
    - *confusion matrix* (матрица несоответствий) по главной диагонали имеет число совпадений (`i` с `j`) - правильный детект цифры - истинное срабатывание, а остальные элементы в основном ноль (в общем случае - сколько раз детектилось `j`, хотя на самом деле было `i`) - срабатывания ложные
    - эта матрица квадратная, кол-во ее строк/столбцов равно числу категорий классификатора. В простейшем случае ("да"/"нет") их две, в случае цифр - 10
    - сумма элементов главной диагонали (правильные срабатывания) называется *следом матрицы*. Желательно, чтобы она была высока по сравнению с суммой остальных элементов
    - можно выводить некие "отладочные" изображения цифр (то, как их видит программа), распознавать и использовать снова в обучении модели, но не совсем понятно как (?)
  * неконтролируемое - на неразмеченном
    - большие датасеты часто неразмечены, поэтому исследуют или распределение данных, или их структуру и значения
    - переменные в датасете делятся на *observable* (наблюдаемые) и *latent* (скрытые). Сведение большого числа наблюдаемых переменных к меньшему числу скрытых - очень полезный навык: снижается размерность, упрощается работа
    - PCA, Principal Components Analysis (метод главных компонент) предназначен как раз для поиска скрытых переменных. Дается пример на Python, исследование качества вина. Библиотека Scikit-learn, датасет от Калифорнийского университета в Ирвайне (UCI)
    - *scree plot* ("график каменистой осыпи") демонстрирует вклад каждой (скрытой) переменной, и является неотъемлемой частью PCA
    - выделенным из этого графика компонентам нужно дать имена, и вероятно для этого требуются экспертные знания в предметной области
    - строим фрейм данных Pandas с новыми (скрытыми) переменными, и проверяем их в прогнозировании, через байесовский классификатор из того же sklearn, как в пред. примере
    - другим методом является *кластеризация*. В модуле sklearn.cluster есть несколько алгоритмов, включая метод k-средних, алгоритм affinity popogation (распространения близости) и спектральныю кластеризацию. Есть (тоже там?) и методы иерархической кластеризации. Недостатком является то, что во всех методов нужно задавать число кластеров заранее, и потом идти методом проб и ошибок
    - пример кластеризации метод k-средних датасета с ирисами, смешивающий неконтролируемое и контролируемое обучение (?)
  * смешанный вариант, частично контролируемое
    - когда меток в датасете очень мало
    - в этом случае можно применить метод *распространения меток* - кластеризовать переменные и назначить всем в кластере то же значение, что и у размеченной
    - еще какой-то метод *активного обучения*, когда алгоритм итеративно указывает, какие наблюдения должны быть помечены на следующем шаге. Или что-то в таком роде

#### Работа с данными на одном компьютере
- основными проблемами при работе с big data являются нехватка памяти, работающие вечно процессы, "узкие места" (одни компоненты перегружены, другие простаивают) и низкая скорость
- решения этих проблем делятся на три группы, три правильных выбора
  * алгоритма. Такой алгоритм не обязан загружать весь датасет в память
    - онлайновые алгоритмы, подход "использовал и забыл". Пример перцептрона на одной NumPy, обучение по эпохам. Бывают и полно- и мини-пакетные, когда данные поступают частями. Отличаются от потоковых алгоритмов тем, что могут несколько раз получать доступ к одним и тем же данным
    - блочные алгоритмы базируются на том, что действия с матрицами (сложение, умножение, инверсия и сингулярное разложение) можно проводить по частям: разбить матрицы на блоки и оперировать уже ими. Такие алгоритмы как PCA и линейная регрессия могут быть представлены в матричной форме. Библиотеки Python: bcolz оптимизирует хранение массивов данных, Dask - оптимизирует последовательность вычислений и распараллеливает их. Пример с этими библиотеками
    - MapReduce, отображение-свертка. Сначала производит отображение набора значений на ключ (map), а потом объединение знаяений по этому ключу (reduce). Б*о*льшую часть работы на себя берут библиотеки Python, такие как Hadoopy, Octopy, Disco, Dumbo
  * структуры данных
    - разреженные данные, напр., матрицы, где большинство элементов равно 0. Часто получаются в реальных задачах (напр., в сообщениях твиттер, в формате "содержит"/"не содержит" слово, почти везде будет "не содержит", 0), и занимают много памяти. Их можно сжимать, напр., до списка кортежей `[(<row>, <col>, <val>), ...]`. Поддержка разреженных матриц в Python растет
    - деревья существенно ускоряют поиск/выборку данных. Используются, например, для построения индексов баз данных
    - хеш-таблицы также ускоряют выборку и также используются для индексов БД. Близкие родственники хранилищ "ключ-ззначение". Пишет, что ключи объединяются в какие-то *buckets* ("гнезда")
  * инструментов, зд. для Python
    - Cuthon, статическая типизация
    - Numexpr, аналог NumPy (там данные в памяти) для больших данных. Использует JIT-компиляцию
    - Numba - просто JIT-компиляция Python-кода
    - Bcolz решает проблемы NumPy при использовании больших данных. Сжимает массивы, неявно используя Numexpr
    - Blaze неявно преобразует данные в стиле Python в SQL, в формат реляционных баз данных или других, типа CSV или Spark
    - Theano, символические вычисления и тензоры. Поддержка GPU и JIT-компиляции
    - Dask - оптимизация последовательности вычислений и их распараллеливание
    - кроме того, Python имеет интерфейсы ко многим популярным программным продуктам
- общие рекомендации при работе с big data в основном те же, что и для общего программирования
  * не повторяйте уже сделанную работу. Используйте базы данных и готовые (высокооптимизированные) библиотеки
  * используйте все возможности оборудования. Передавайте процессору сжатые данные, используйте видеокарту, используйте многопоточные параллельные вычисления
  * экономьте вчислительные ресурсы. Профилируйте и оптимизируйте узкие места, используйте откомпилированный код (из пакетов или свой, через JIT/другие языки), избегайте загрузки всех данных в память (читайте их блоками/последовательно), используйте генераторы для отказа от промежуточного хранения данных (а это не то же, что пред.?), используйте как можно меньший объем данных (?), преобразовывайте математические выражения (пример с квадратом сумммы)
- пример 1, прогнозирование вредоносных урлов
  * SciKit-learn и большой внешний датасет уже на стадии загрузки вызывают нехватку памяти
  * простой строчкой кода на Python проверяем, что данные содержат всего десятитысячные доли процента ненулевых значений
  * т.о. для экономии памяти работаем сразу с разреженным данными - что-то там про формат SVMLite, который их по дефоллту поддерживает. Кроме того, через библиотеку `tarfile` перебираем файлы в датасете, не распаковывая его
  * используем (опять же, для экономии памяти) онлайновый алгоритм стохастического градиентного спуска, вычитывая данные из файлов по частям, через `partial_fit()`
  * функция `classification_report()` выводит таблицу с точностью, полнотой и прочим
- пример 2, рекомендательная система внутри базы данных
  * MySQL (установка описана в прил. B) и библиотека Python для интеграции с ней (используется MySQLdb, но можно и SQLAlchemy)
  * использует метод k ближайших соседей, локально-чувствительное хеширование (LSH), и расстояние Хемминга для строк
  * в матрице (в таблице БД) "клиент"/"смотрел ли фильм" столбцы фильмов (0/1) можно объединить в один, в битовое поле (`1100101100...`). Это преобразование он и называет хеш-функцией (?)
  * в процессе работы с данными зачем-то используются фреймы Pandas
  * для ускорения работы прямо через Python строит индекс на таблице
  * вычисление расстояния Хемминга реализует в виде хранимой процедуры

#### Введение в big data
- в предыдущей главе рассматривался случай когда данные уже не помещались в память, но всё еще могли быть обработаны на одном компьютере. В этой рассматриваются такие, которые на одном компе обработать уже нельзя
- Hadoop
  * состоит из распределенной файловой системы HDFS, системы выполнения распределнных программ MapReduce и менеджера ресурсов YARN
  * на этой базе возникла экосистема приложений: базы Hive и HBase, инфраструктура машинного обучения Mahout, ускоритель для Hive - Impala, и еще десяток (картинка, [p.153])
  * MapReduce
    - предполагается, что исходные данные состоят из частей, на которые их можно разбить - ключей. В примере из книги это цвета
    - в фазе отображения (map) документы делятся на пары "ключ" (цвет) / "значение" (кол-во вхождений в исходном документе) - ключ отображается на значение. Т.к. ключи повторяются в документах, в сумме появляется много повторений. В этом процессе каждая строка документа передается в *mapper job* (задание отображения)
    - ключи сортируются
    - в фазе свертки (reduce) кол-во вхождений суммируется (аналог `GROUP BY`), и выводится в единый для всех документов файл
- Hadoop часто бывает труден в настройке и управлении для небольших компаний, поэтому в книге используется готовый Horton Sandbox
- Hadoop записывает данные на диск между этапами вычислений и поэтому плохо подходит для интерактивных алгоритмов. Для предоления этого был создан Spark
- Spark
  * базируется на HDFS, YARN или Apache Mesos (менеджер кластеров), не реализует всё это сам
  * создает RDD (Resilient Distributed Datasets) - распределенную абстракцию памяти: в некотором роде эмулирует общую для всех узлов кластера память, позволяя хранить в ней переменные и их состояния, что делает ненужным запись промежуточных результатов на диск
  * его ядро представляет собой среду NoSQL, компоненты: Spark Streaming (анализ в реальном времени), Spark SQL (интерфейс к Spark через SQL), MLLib (инструмент машинного обучения), GraphX (графовая база данных)
- пример, оценка риска при кредитовании
  * запускаем Horrton Sandbox в VirtualBox-е, подключаемся к нему через PuTTY, ставим pywebhdfs и Pandas
  * опять же через PuTTY работаем с HDFS, напр.: `$ hadoop fs -ls -R`. Создаем каталоги. Команды похожи на обычные консольные, но перед каждой пишется `hadoop fs` и ставится дефис. Команды `-put`/`-get` обмениваются данными между локальной и распределенной файловой системами
  * в целом, HDFS для пользователя похожа на обычную ФС, только скрывает реальное расположение (и видимо дробление) файлов
  * команда `$ pyspark` запустит REPL, куда копируем код. Все действия дальше идут через Python
  * качаем датасет Lending Club-а, распаковываем, функцией из пакета pywebhdfs закачиваем CSV-шки в HDFS
  * ~~хоть REPL и pyspark, т.е. взаимодействует со Spark, к нему еще нужно специально подключаться (`from pyspark import SparkContext` и так же для Hive)~~ контекст `sc` присутствует автоматически, это если через веб-блокнот Zeppelin подключаться, нужно явно создавать. То же и для Hive (переменная `sqlContext`)
  * дальше разбираем загруженный CSV: `sc.textFile('./...')`, что-то там очищается
  * через методы контекста Hive создаются датафреймы, таблицы и загружаются данные. Можно и не через методы, а через SQL-синтаксис, вызовом `sqlContext.sql(...)`
  * с этими сохраненными данными теперь можно работать через HiveQL - диалект SQL. Создаем таблицу в формате Paraquet - популярном формате файлов big data: `create table raw stored as paraquet`
  * для построения отчета скачиваем программу Qlik Sense, через ODBC коннектим ее к Hive к таблице `raw`, выбираем для импорта все столбцы и визуальными средствами в Qlik создаем "приложение" - отчет с диаграммами, сводной таблицей и интерактивным графиком

#### NoSQL
- NoSQL = Not Only SQL, не только SQL. Многие из них дают возможность делать запросы на языке SQL
- появились потому, что традиционные РСУБД существуют на одном компьютере, а нужна была масштабируемость. Хотя многие используют NoSQL-базы и на одном узле, из-за их гибкости или иерархичности, напр., MongoDB. В целом, важно каждое из четырех V
- позже появились и масштабируемые РСУБД - NewSQL, предоставляющие кластеризацию и распараллеливание, подвидом которых можно считать и Hive
- четыре типа баз NoSQL: хранилища документов (MongoDB, ElasticSearch, CouchDB), хранилища "ключ-значение" (Redis, Memcache, Voldemort, Riak, Amazon Dynamo), столбцовые БД (Apache HBase, HyperTable, Facebook Cassandra, Google BigTable), графовые БД (Neo4J)
- ACID - базовые принципы реляционных БД: *Atomicity* (атомарность) - операции не могут быть выполнены наполовину, *Consistency* (согласованность) - целостность данных (ключи, индексы, обязательные поля и типы), *Isolation* (изолированность) - последовательность внесения изменений, *Durability* (долгосрочность) - долговременное хранение, операции записи сохраняют данные на диск, не в оперативку
- теорема CAP - *распределнная* БД может обладать любыми двумя характеристиками, но не всеми тремя сразу: *Consistency* (согласованность) - получение одних и тех же данных от любого узла, *Availability* (доступность) - если узел работает, то он досупен (??); узел отвечает независимо от других узлов, *Partition tolerance* (устойчивость к разделению) - база может справится с сетевой сегментацией или сетевым сбоем
- то есть, по CAP: если сеть сегментирована (идет сбой синхронизации), то приходится выбирать между доступностью (нужно отвечать) и согласованностью (нельзя синхронизировать всех при сбое). Пример с одной единицей товара в интернет-магазине (тут, как правило, предпочитают доступность: проще извиниться перед одним, чем потерять обе продажи) и билетами на фестиваль (предпочитают согласованность: проще отключить ненадолго продажи, чем продав больше чем нужно билетов, возвращать деньги за тысячи их)
- (?) BASE - базовые принципы реляционных NoSQL-БД: *Basically Available* (базовая доступность) - доступность в смысле CAP, *Soft state* (неустойчивое состояние) - состояние может изменится со временем, *Ecentual consistency* (согласованность в конечном счете) - так что согласованность наступит не сразу, но с течением времени, по какой-либо стратегии
- реляционные базы данных (РБД) стремятся к нормализации, т.е. чтобы не дублировать информацию в таблицах, а лишь соединять их по ключам. Они также являются строково-ориентированными, при поиске по ним в память вычитываются все столбцы каждой записи
- в столбцовых базах все столбцы хранятся по отдельности, с указанием соответствующих номеров строк. Это похоже на РБД с индексированием по каждому столбцу. Такие базы предпочтительней для аналитики - быстрого суммирования/агрегирования, подсчета значений, но медлены, когда нужно вставить/изменить запись: нужно изменять все таблицы столбцов этой записи. В последнем случае предпочтительней РБД
- OLTP (Online Transaction Processing) - оперативная обработка транзакций, подразумевает частое добавление и/или обновление записей, тут выбирают РБД. Для быстрой аналитики и выдачи отчетов выбирают столбцовые БД, возможно пакетно обновляя ее записи по ночам
- хранилища "ключ-значение" имеют самую простую архитектуру, и потому лучше всех масштабируются
- хранилища документов чуть сложнее - уже подразумевают структуру хранящихся данных, их *схему*. Не заботятся о нормализации, они наиболее "естественно" выглядят. Пример с газетной статьей - в РБД ее нужно "нарезать" на поля (текст, автор и т.д.), а здесь она хранится как единая сущность, пусть и в виде структурированного JSON-а
- графовые БД самые сложные, состоят из компонентов двух типов: узлов и ребер (могут быть направленными). Они уже претендуют на соблюдение принципов ACID, тогда как два пред. хранилища - только на BASE
- пример, диагностика болезней
  * используется пакет elasticsearch-py - хранилище документов с полнотекстовым поиском. Ее аналог, выросший из того же Apache Lucene, это Apache Solr - поисковая система корпоративного уровня, несколько более сложная
  * данные получаем из википедии по API, через Python-пакет `wikipedia`. Создается клиент Elasticsearch, создается схема данных, по API запрашиваются страницы о болезнях (начиная со страницы-списка и ссылок на ней), и строится полнотекстовый индекс
  * всякими сомнительными эвристиками фильтруют ссылки, выделяя страницы именно болезней. Обращаются к индексу через `http://localhost:9200/<index-name>/...` (дефолтный порт Elasticsearch), получая JSON. Поисковые запросы делаются через POST
  * Elasticsearch обладает довольно сложным языком запросов, в котором имеются *запросы*, *фильтры* и *обобщения*. Но в этом примере используется простой язык, напоминающий язык запросов Google, с `+` перед обязательными условиями и `-` перед исключением из условия, `^` - для назнаяения веса. Считается что он достаточно медленный, но здесь быстродействие не играет роли
  * по запросам вида `"query": '+fatigue+fever+"joint pain"+rash'`возвращается список json-объектов - болезней, с названием и разными параметрами, типа `score` (ранжирует результаты поиска)
  * расстояние Дамерау-Левенштайна определяет кол-во операций, необходимых для преобразования одной строки в другую. Его можно испоьзовать для коррекции опечаток в запросах (напр., `"Lupsu" --> "Lupus"` - одна перестановка), но не показано как
  * фильтруем все записи индекса при помощи быстрого объекта-фильтра (не ранжирует записи (`score: 0`), просто отсекает), ищем ключевые слова для `"diabetes"`, приходим к выводу что сишком много лишних и нужно хранить *диграммы* - пары слов, характерные для естественного языка. Для этого возвращаемся назад, как это нередко бывает в DS
  * в Elasticsearch уже встроен настраиваемый фильтр n-грамм, так и анализатор для них. Перегенерируем индекс и снова ищем ключевые слова для `"diabetes"`
  * визуализировать облако (ключевых) слов можно через Python-пакет word_cloud или JS-библиотеку D3.js. Или создать сайт на Django, как прослойку между Elasticsearch и веб-интерфейсом (у ES нет бесплатных средств безопасности)

#### Графовые базы
- яркий пример - соцсети: *сущности* (люди, пользователи) - узлы, *свойства* (напр., "имя: Джелме") - пары ключ/значения для сущностей, *отношения* (напр., "знает") - ребра, могут быть направленными, и *метки* (напр., группы пользователей) используются для группировки
- следует использовать их, когда данные (относительно) невелики, но достаточно сложны. Примеры графовых БД: Neo4j, OrientDB, Titan
- Cyther - язык запросов к графам. Используется для вставки данных в Neo4j, как через интерфейс самой СУБД, так и через Python. Идет описание и примеры его синтаксиса
- пример, рекомендательная система
  * в рекомендательных системах вообще часто используются графовые БД
  * с гитхаба беут два файла: возможные ингредиенты в формате .txt и большую подборку рецептов в .json
  * парсят JSON и заполняют индекс Elasticsearch тысячами рецептов (для некой очистки данных, ???	)
  * Python-пакет py2neo используется для заполнения базы Neo4j списками ингредиентов и рецептов
  * с использование Cyther, прямо в Python (хотя можно и в СУБД), выполняют запросы и ищут самые часто встречающиеся ингредиенты, какие рецепты требуют больше всего ингредиентов, и прочую статистику
  * добавляют в базу людей, их отношение Likes к узлам рецептов, и запросами Cyther ищут блюда, которые им могут понравится (по ингредиентам)
  * веб-интерфейс Neo4j позволяет выводить красивые графы

#### Глубокий анализ текста (text mining)
- первым шагом здесь является структурирование входного текста - перевод в табличную форму. Пример с сообщениями о преступллениях - из них выделаются поля "преступник", "жертва", "тип преступления" и т.д., формуируется таблица
- *bag of words* (набор слов) - способ структурирования текста. Рассматриваются несколько "документов" (сообщений) одновременно. Составляется матрица "документ-термин", где столбцы - это слова, строки - "документы", а значения ячеек - true/false в зависимости от того, встречается ли слово в конкретном сообщении. В этом простом примерене применялось ни выделение основы, ни фильтрация слов
- TF-IDF (Term Frequency - Inverse Document Frequency) - формула, часто применяемая для заполнения матрицы "документ-термин". Т.к. чем чаще встречается слово вообще ((I)DF), тем меньше оно несет информации (меньше его вес), поэтому частоту слова TF можно умножить на IDF, получив итоговый вес. При расчете IDF используется логарифмическое масштабирование, оно же может использоваться и для TF. Метрика TF-IDF неявно используется в Elasticsearch
- перед построением матрицы происходит также несколько других операций: разбиение текста на лексемы (в нашем случае на униграммы, слова), фильтрация игнорируемых слов (как правило, слишком частых), преобразование к нижнему регистру
- *stemming* (выделение основы) снижает вариативность текста. Отсекает, напр., множественное число существительных
- *lemmatization* (лемматизация) преследует ту же цель, но делает это более осмысленно с точки зрения грамматики (напр., `"are" == "be"`). Для нее может потребоваться *POS tagging (Part of speech tagging)* (разметка частей речи) - составление списка пар "слово-аббревиатура части речи" (напр., "{"game": "NN"}). Таблица аббревиатур для частей речи, [p.267]
- вместо наивного байеса (т.к. слова в диграммах и триграммах часто не независимы) предлагается использовать дерево решений. Что-то там про энтропию и недостаток решающего дерева - на уровне листов происходит отсчесение части входных данных
- пример, классификация сообщений Reddit
  * ищут принадлежность слов сабреддиту, про "Игру престолов" или про Data Science
  * хотя у NLP есть много применений, в этом примере рассматривается только классификация
  * используется Python-пакет nltk. Для полноценной работы ему требуется загружать модели и корпуса текстов: `import nltk; nltk.download()`
  * также в примере используются библиотеки praw (для загрузки сообщений с Rediit), SqlLite3 (для хранения) и Matplotlib (для вывода отчетов)
  * очистка данных: корпусы nltk уже содержат списки игнорируемых (слишком частых) слов. + приведение к нижнему регистру. Стандартнрй функцией из nltk нарезаем текст на лексемы
  * в фазе исследования данных строят столбчатые гистограммы (по частоте вхождения слов) и ищут *гапаксы* - термины, встречающиеся в одном документе один раз. У nltk есть функция их выделения, `.hapaxes()`
  * возвращаемся к фазе очистки, удаляем односимвольные слова - знаки препинания, при помощи адгоритма SnowBall из nltk выделяем основы слов
  * выделение основы превращает, напр., слова "course"/"courses" в "cours", и т.д., но их смысл всё еще нетрудно понять. Если это неприемлемо, то можно воспользоваться лемматизацией
  * создается большая разреженная матрица "документ-термин", с `True`/`False` в ячейках. Здесь она помещается в память, и в методах работы с большими данными необходимости нет
  * делят датасет на обучающую и контрольную выборки в пропорции 3:1, и обрабатывают двумя способами: наивным байесом (у nltk есть свой) и деревом принятия решений (строится как серия `if`-ов проверки самых "весомых" слов сабреддита)
  * проверяют матрицу несоответствий, смотрят (методами nltk) веса отдельных слов
  * при помощи библиотеки d3.js строят *force graph* (силовой граф) - тема сабреддита в центре, а размеры кружков и связей для узлов показывают их веса (значимость)
  * приводится и другой пример визуализации отчета - кольцевая диаграмма (диаграмма "солнечные лучи")

#### Визуализация данных
- JS библиотеки: CrossFilter (MapReduce для JavaScript), d3.js (визуализация) и dc.js - некий их гибрид
- сервер запускают через Python, из командной строки: `python -m SimpleHTTPServer`. Запускают в папке с html и js/css файлами
- данные берут из предварительного сформированного CSV-файла, хотя в реальной жизни это была бы REST-служба. Загружают его методом `d3.csv(...)`
- в том же JS-коде, после загрузки, динамически рисуют таблицы. Данные для этих таблиц можно фильтровать и группировать (MapReduce) средствами библиотеки CrossFilter. Можно передавать и кастомные функции в ее `.reduce()`, вычисляя, например, среднее арифметическое
- средствами библиотеки dc.js рисуются интеракивные графики (`dc.lineChart("<selector>")`), строковые диаграммы (`dc.rowChart("<selector>")`) или секторные (`dc.pieChart("<selector>")`) диаграммы
- существуют и программы для визуализации, такие как Tableau или Qlik, а также другие JS-библиотеки. Для последних их бесплатность, высокая кастомизуемость и знакомство многих людей с веб-технологиями, могут стать преимуществами перед десктопными программами типа Tableau
- и вообще пишет, что визуализация - чуть не главный момент для data scientist-а: донесение выводов до бизнес-стороны

#### Приложения
- установка и настройка Elasticsearch
- установка Neo4j
- установка MySQL
- установка Anaconda

---

- установка, настройка и работа с Hadoop, HiveQL, Spark
- настройка conda
- да и все остальные программы и инструменты, упоминаемые в книге
- примеры на Python [pp.73,75-76,94-97,102-107,110,118-122,124-125,135-138,143-149,205-213;223,246-253,275-291]
- байесовская вероятность и правило
- библиотеки Python, примеры на которые есть в книге
